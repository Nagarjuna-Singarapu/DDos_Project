{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "487ab284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.89% \n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Best solver is :  liblinear\n",
      "------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.75      0.73     11587\n",
      "           1       0.76      0.73      0.74     12756\n",
      "\n",
      "    accuracy                           0.74     24343\n",
      "   macro avg       0.74      0.74      0.74     24343\n",
      "weighted avg       0.74      0.74      0.74     24343\n",
      " \n",
      "\n",
      "------------------------------------------------------------------------\n",
      "--- 0.9074513912200928 seconds --- time for LogisticRegression\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.03, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.03, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=0.03, solver='liblinear')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import resample\n",
    "from sklearn import metrics\n",
    "\n",
    "data = pd.read_csv('dataset_sdn.csv')\n",
    "\n",
    "##### Here we see that the label contains boolean values: 0 - Benign, 1-Maliciuous \n",
    "data.label.unique()\n",
    "\n",
    "data.label.value_counts()\n",
    "\n",
    "#label_dict = dict(data.label.value_counts())\n",
    "#sns.countplot(x=data.label)\n",
    "# plt.show()\n",
    "\n",
    "'''labels = [\"Maliciuous\",'Benign']\n",
    "sizes = [dict(data.label.value_counts())[0], dict(data.label.value_counts())[1]]\n",
    "plt.figure(figsize = (13,8))\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "plt.legend([\"Maliciuous\", \"Benign\"])\n",
    "plt.title('The percentage of Benign and Maliciuos Requests in dataset')\n",
    "plt.show()\n",
    "\n",
    "#data.describe()\n",
    "\n",
    "#data.isnull().sum()\n",
    "\n",
    "#### Let's support which columns NUMERIC and which is OBJECT\n",
    "\n",
    "numeric_df = data.select_dtypes(include=['int64', 'float64'])\n",
    "object_df = data.select_dtypes(include=['object'])\n",
    "numeric_cols = numeric_df.columns\n",
    "object_cols = object_df.columns\n",
    "print('Numeric Columns: ')\n",
    "print(numeric_cols, '\\n')\n",
    "print('Object Columns: ')\n",
    "print(object_cols, '\\n')\n",
    "print('Number of Numeric Features: ', len(numeric_cols))\n",
    "print('Number of Object Features: ', len(object_cols))'''\n",
    "\n",
    "# Assuming 'data' is the original dataframe\n",
    "# Separate the data based on unique label values\n",
    "df_label_0 = data[data['label'] == 0]\n",
    "df_label_1 = data[data['label'] == 1]\n",
    "\n",
    "# Resample to get equal number of rows for each label value\n",
    "df_label_0_resampled = resample(df_label_0,\n",
    "                                replace=False, # sample without replacement\n",
    "                                n_samples=len(df_label_1), # sample same number of rows as label 1\n",
    "                                random_state=42) # set random state for reproducibility\n",
    "\n",
    "# Combine the resampled dataframes\n",
    "balanced_data = pd.concat([df_label_0_resampled, df_label_1])\n",
    "\n",
    "# Shuffle the rows to avoid any ordering bias\n",
    "data = balanced_data.sample(frac=1, random_state=42)\n",
    "\n",
    "\n",
    "data.label.value_counts()\n",
    "\n",
    "df1 = data.copy()\n",
    "\n",
    "df1 = df1.dropna()\n",
    "\n",
    "df1.columns\n",
    "\n",
    "#df1.info()\n",
    "\n",
    "important_features = [\n",
    "    'src',\n",
    "    'pktcount',\n",
    "    'dst',\n",
    "    'byteperflow',\n",
    "    'pktperflow',\n",
    "    'pktrate',\n",
    "    'tot_kbps',\n",
    "    'rx_kbps',\n",
    "    'flows',\n",
    "    'bytecount',\n",
    "    'dt',\n",
    "    'Protocol',\n",
    "    'dur',\n",
    "    'tot_dur'\n",
    "                      \n",
    "                     ]\n",
    "\n",
    "\n",
    "weights = [\n",
    "    17.87,\n",
    "    15.16,\n",
    "    13.64,\n",
    "    12.97,\n",
    "    11.35,\n",
    "    11.35,\n",
    "    9.68,\n",
    "    9.66,\n",
    "    8.95,\n",
    "    4.92,\n",
    "    2.33,\n",
    "    1.31,\n",
    "    1.11,\n",
    "    1.11\n",
    "]\n",
    "\n",
    "weighted_features = pd.DataFrame({'features':important_features,\n",
    "                                 'weights':weights})\n",
    "weighted_features\n",
    "\n",
    "### But we dont need src, dst, dt, So, we will drop them\n",
    "X = df1[important_features]\n",
    "y = df1.label\n",
    "\n",
    "X = X.drop(['src', 'dst', 'dt'], axis=1)\n",
    "\n",
    "X.head()\n",
    "\n",
    "abs(X.corr(numeric_only=True))\n",
    "\n",
    "X = X.drop(['dur', \"pktrate\", \"pktperflow\"], axis=1)\n",
    "\n",
    "X.columns\n",
    "\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, data, y):\n",
    "        self.data = data\n",
    "        self.y = y\n",
    "        X = preprocessing.StandardScaler().fit(self.data).transform(self.data)\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, self.y, random_state=42, test_size=0.3)  \n",
    "        \n",
    "\n",
    "    def LogisticRegression(self):\n",
    "        solvers = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "\n",
    "        start_time = time.time()\n",
    "        results_lr = []\n",
    "        accuracy_list = []\n",
    "        for solver in solvers:\n",
    "            LR = LogisticRegression(C=0.03, solver=solver).fit(self.X_train, self.y_train)\n",
    "            predicted_lr = LR.predict(self.X_test)\n",
    "            accuracy_lr = accuracy_score(self.y_test, predicted_lr)\n",
    "            #print(\"Accuracy: %.2f%%\" % (accuracy_lr * 100.0))\n",
    "            #print('################################################################')\n",
    "            results_lr.append({'solver' : solver, 'accuracy': str(round(accuracy_lr * 100, 2)) + \"%\", \n",
    "                                  'Coefficients': {'W' : LR.coef_, 'b': LR.intercept_}})\n",
    "            \n",
    "            accuracy_list.append(accuracy_lr)\n",
    "       \n",
    "        solver_name = solvers[accuracy_list.index(max(accuracy_list))]\n",
    "        LR = LogisticRegression(C=0.03, solver=solver_name).fit(self.X_train,self.y_train)\n",
    "        predicted_lr = LR.predict(self.X_test)\n",
    "        accuracy_lr = accuracy_score(self.y_test, predicted_lr)\n",
    "        print(\"Accuracy: %.2f%%\" % (accuracy_lr * 100.0), '\\n')\n",
    "        print(\"------------------------------------------------------------------------\")\n",
    "        print('Best solver is : ', solver_name)\n",
    "        print(\"------------------------------------------------------------------------\")\n",
    "        print(classification_report(predicted_lr, self.y_test), '\\n')\n",
    "        print(\"------------------------------------------------------------------------\")\n",
    "        print(\"--- %s seconds --- time for LogisticRegression\" % (time.time() - start_time))\n",
    "    \n",
    "        return LR\n",
    "\n",
    "\n",
    "M = Model(X,y)\n",
    "M.LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e1f312",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
